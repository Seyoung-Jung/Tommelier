{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCN_DATASET_modeling_jounyoung.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "u2rBBLBhXYee",
        "WOfyFAUhXU6J"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7IjBfIFG7RL"
      },
      "source": [
        "<코드참조>\r\n",
        "https://www.tensorflow.org/recommenders/examples/dcn\r\n",
        "\r\n",
        "https://www.tensorflow.org/datasets/catalog/movie_lens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9-1XilglpdL"
      },
      "source": [
        "!pip install -q tensorflow-recommenders\r\n",
        "!pip install -q --upgrade tensorflow-datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcHkN-Lolh6v"
      },
      "source": [
        "import pprint\r\n",
        "\r\n",
        "%matplotlib inline\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "import tensorflow_datasets as tfds\r\n",
        "\r\n",
        "import tensorflow_recommenders as tfrs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dg8uxRTOsXxY"
      },
      "source": [
        "import json\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDgqAWI7mtCD"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBy1DmvR8ULM"
      },
      "source": [
        "# 램초과\r\n",
        "wine_raw = '/content/drive/MyDrive/tobigs14_conference/data/v_2/wine_meta/train_all_meta_v2.json'\r\n",
        "wine_raw = pd.read_json(wine_raw)\r\n",
        "wine_raw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSDOT6xpvS1i"
      },
      "source": [
        "wine_meta = '/content/drive/MyDrive/tobigs14_conference/data/v_2/wine_meta/Wine_Meta_final_201210.json'\r\n",
        "wine_meta = pd.read_json(wine_meta)\r\n",
        "wine_meta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRF2iTDxwXAl"
      },
      "source": [
        "wine_meta_rank=wine_meta[['wine_id','world_rank', 'type_id', 'country_code']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2rBBLBhXYee"
      },
      "source": [
        "# train set 만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3fXhouf4LVc"
      },
      "source": [
        "user_train = '/content/drive/MyDrive/tobigs14_conference/data/v_2/userdata/user_train.json'\r\n",
        "user_train = pd.read_json(user_train)\r\n",
        "user_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56rVfU0-wsWJ"
      },
      "source": [
        "user_train=pd.merge(user_train, wine_meta_rank)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67kzSLSixPGC"
      },
      "source": [
        "user_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UebanLJ2ZO49"
      },
      "source": [
        "user_train={\"wine_id\": user_train[\"wine_id\"],\r\n",
        "    \"userID\": user_train[\"userID\"],\r\n",
        "    \"rating_per_user\": user_train[\"rating_per_user\"],\r\n",
        "    \"user_like_count\": user_train[\"user_like_count\"],\r\n",
        "    \"type_id\": user_train[\"type_id\"],\r\n",
        "    \"world_rank\": user_train[\"world_rank\"]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eC023w1Uoq6"
      },
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices(user_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmeX_iAIUp0t"
      },
      "source": [
        "train_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNnjaQsG49NG"
      },
      "source": [
        "user_train_1 = train_dataset.map(lambda x: {\r\n",
        "    \"wine_id\": x[\"wine_id\"],\r\n",
        "    \"userID\": x[\"userID\"],\r\n",
        "    \"rating_per_user\": x[\"rating_per_user\"],\r\n",
        "    \"type_id\": x[\"type_id\"],\r\n",
        "    \"user_like_count\": x[\"user_like_count\"]\r\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ul-pg6NyTanu"
      },
      "source": [
        "user_train_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0n7o9ZXPq0q"
      },
      "source": [
        "tf.random.set_seed(42)\r\n",
        "shuffled = user_train_1.shuffle(937756, seed=42, reshuffle_each_iteration=False)\r\n",
        "\r\n",
        "train = shuffled.take(937756)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dk3kODp2Banf"
      },
      "source": [
        "feature_names = [\"wine_id\", \"userID\", \"rating_per_user\", \"type_id\", \"user_like_count\"]\r\n",
        "\r\n",
        "vocabularies = {}\r\n",
        "\r\n",
        "for feature_name in feature_names:\r\n",
        "  vocab = user_train_1.batch(937756).map(lambda x: x[feature_name])\r\n",
        "  vocabularies[feature_name] = np.unique(np.concatenate(list(vocab)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvDcWkAV7OBc"
      },
      "source": [
        "user_train_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOfyFAUhXU6J"
      },
      "source": [
        "# testset 만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J73PnGyBXT5f"
      },
      "source": [
        "user_test = '/content/drive/MyDrive/tobigs14_conference/data/v_2/userdata/user_test.json'\r\n",
        "user_test = pd.read_json(user_test)\r\n",
        "user_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWWPQJylAzGf"
      },
      "source": [
        "user_test=pd.merge(user_test, wine_meta_rank)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GG0yKXAzXvVM"
      },
      "source": [
        "user_test={\"wine_id\": user_test[\"wine_id\"],\r\n",
        "    \"userID\": user_test[\"userID\"],\r\n",
        "    \"rating_per_user\": user_test[\"rating_per_user\"],\r\n",
        "    \"type_id\": user_test[\"type_id\"],\r\n",
        "    \"user_like_count\": user_test[\"user_like_count\"]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjBVYf5XXdnR"
      },
      "source": [
        "test_dataset = tf.data.Dataset.from_tensor_slices(user_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O76lh8PSY6E5"
      },
      "source": [
        "test_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9S4TarPAYztT"
      },
      "source": [
        "user_test_1 = test_dataset.map(lambda x: {\r\n",
        "    \"wine_id\": x[\"wine_id\"],\r\n",
        "    \"userID\": x[\"userID\"],\r\n",
        "    \"rating_per_user\": x[\"rating_per_user\"],\r\n",
        "    \"type_id\": x[\"type_id\"],\r\n",
        "    \"user_like_count\": x[\"user_like_count\"]\r\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDEyVVr_Zh2G"
      },
      "source": [
        "user_test_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3btZxHk8ZgcS"
      },
      "source": [
        "tf.random.set_seed(42)\r\n",
        "shuffled = user_test_1.shuffle(6343 , seed=42, reshuffle_each_iteration=False)\r\n",
        "\r\n",
        "test = shuffled.take(6343)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuNtzZxCiS1C"
      },
      "source": [
        "user_test_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8bCXWy1GvK-"
      },
      "source": [
        "# 모델 트레이닝"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EImUtXG-WKHZ"
      },
      "source": [
        "class DCN(tfrs.Model):\r\n",
        "\r\n",
        "  def __init__(self, use_cross_layer, deep_layer_sizes, projection_dim=None):\r\n",
        "    super().__init__()\r\n",
        "\r\n",
        "    self.embedding_dimension = 32\r\n",
        "\r\n",
        "    str_features = []\r\n",
        "    int_features = [\"userID\", \"type_id\",\"user_like_count\"]\r\n",
        "\r\n",
        "    self._all_features = str_features + int_features\r\n",
        "    self._embeddings = {}\r\n",
        "\r\n",
        "    # Compute embeddings for string features.\r\n",
        "    for feature_name in str_features:\r\n",
        "      vocabulary = vocabularies[feature_name]\r\n",
        "      self._embeddings[feature_name] = tf.keras.Sequential(\r\n",
        "          [tf.keras.layers.experimental.preprocessing.StringLookup(\r\n",
        "              vocabulary=vocabulary, mask_token=None),\r\n",
        "           tf.keras.layers.Embedding(len(vocabulary) + 1,\r\n",
        "                                     self.embedding_dimension)\r\n",
        "    ])\r\n",
        "      \r\n",
        "    # Compute embeddings for int features.\r\n",
        "    for feature_name in int_features:\r\n",
        "      vocabulary = vocabularies[feature_name]\r\n",
        "      self._embeddings[feature_name] = tf.keras.Sequential(\r\n",
        "          [tf.keras.layers.experimental.preprocessing.IntegerLookup(\r\n",
        "              vocabulary=vocabulary, mask_value=None),\r\n",
        "           tf.keras.layers.Embedding(len(vocabulary) + 1,\r\n",
        "                                     self.embedding_dimension)\r\n",
        "    ])\r\n",
        "\r\n",
        "    if use_cross_layer:\r\n",
        "      self._cross_layer = tfrs.layers.dcn.Cross(\r\n",
        "          projection_dim=projection_dim,\r\n",
        "          kernel_initializer=\"glorot_uniform\")\r\n",
        "    else:\r\n",
        "      self._cross_layer = None\r\n",
        "\r\n",
        "    self._deep_layers = [tf.keras.layers.Dense(layer_size, activation=\"relu\")\r\n",
        "      for layer_size in deep_layer_sizes]\r\n",
        "\r\n",
        "    self._logit_layer = tf.keras.layers.Dense(1)\r\n",
        "\r\n",
        "    self.task = tfrs.tasks.Ranking(\r\n",
        "      loss=tf.keras.losses.MeanSquaredError(),\r\n",
        "      metrics=[tf.keras.metrics.RootMeanSquaredError(\"RMSE\")]\r\n",
        "    )\r\n",
        "\r\n",
        "  def call(self, features):\r\n",
        "    # Concatenate embeddings\r\n",
        "    embeddings = []\r\n",
        "    for feature_name in self._all_features:\r\n",
        "      embedding_fn = self._embeddings[feature_name]\r\n",
        "      embeddings.append(embedding_fn(features[feature_name]))\r\n",
        "    \r\n",
        "    x = tf.concat(embeddings, axis=1)\r\n",
        "\r\n",
        "    # Build Cross Network\r\n",
        "    if self._cross_layer is not None:\r\n",
        "      x = self._cross_layer(x)\r\n",
        "    \r\n",
        "    # Build Deep Network\r\n",
        "    for deep_layer in self._deep_layers:\r\n",
        "      x = deep_layer(x)\r\n",
        "\r\n",
        "    return self._logit_layer(x)\r\n",
        "\r\n",
        "  def compute_loss(self, features, training=False):\r\n",
        "    labels = features.pop(\"wine_id\")\r\n",
        "    scores = self(features)\r\n",
        "    return self.task(\r\n",
        "        labels=labels,\r\n",
        "        predictions=scores,\r\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sy-3SSHjW8b0"
      },
      "source": [
        "cached_train = train.shuffle(100_000).batch(8192).cache()\r\n",
        "cached_test = test.batch(4096).cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQSWpfXgXGIf"
      },
      "source": [
        "def run_models(use_cross_layer, deep_layer_sizes, projection_dim=None, num_runs=5):\r\n",
        "  models = []\r\n",
        "  rmses = []\r\n",
        "\r\n",
        "  for i in range(num_runs):\r\n",
        "    model = DCN(use_cross_layer=use_cross_layer,\r\n",
        "                deep_layer_sizes=deep_layer_sizes,\r\n",
        "                projection_dim=projection_dim)\r\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate))\r\n",
        "    models.append(model)\r\n",
        "\r\n",
        "    model.fit(cached_train, epochs=epochs, verbose=False)\r\n",
        "    metrics = model.evaluate(cached_test, return_dict=True)\r\n",
        "    rmses.append(metrics[\"RMSE\"])\r\n",
        "\r\n",
        "  mean, stdv = np.average(rmses), np.std(rmses)\r\n",
        "\r\n",
        "  return {\"model\": models, \"mean\": mean, \"stdv\": stdv}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPoz20b0Z4Vm"
      },
      "source": [
        "epochs = 8\r\n",
        "learning_rate = 0.01"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8perphpuCo9i"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkccJswRcsG9"
      },
      "source": [
        "dcn_result = run_models(use_cross_layer=True,\r\n",
        "                        deep_layer_sizes=[192, 192])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6h2fsZI4Z5L6"
      },
      "source": [
        "dcn_lr_result = run_models(use_cross_layer=True,\r\n",
        "                           projection_dim=3,\r\n",
        "                           deep_layer_sizes=[192, 192])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkoZ8-rwEAjD"
      },
      "source": [
        "dnn_result = run_models(use_cross_layer=False,\r\n",
        "                        deep_layer_sizes=[192, 192, 192])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkXtPSe6EzKV"
      },
      "source": [
        "print(\"DCN            RMSE mean: {:.4f}, stdv: {:.4f}\".format(\r\n",
        "    dcn_result[\"mean\"], dcn_result[\"stdv\"]))\r\n",
        "print(\"DCN (low-rank) RMSE mean: {:.4f}, stdv: {:.4f}\".format(\r\n",
        "    dcn_lr_result[\"mean\"], dcn_lr_result[\"stdv\"]))\r\n",
        "print(\"DNN            RMSE mean: {:.4f}, stdv: {:.4f}\".format(\r\n",
        "    dnn_result[\"mean\"], dnn_result[\"stdv\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGR2-6R0E2QF"
      },
      "source": [
        "model = dcn_result[\"model\"][0]\r\n",
        "mat = model._cross_layer._dense.kernel\r\n",
        "features = model._all_features\r\n",
        "\r\n",
        "block_norm = np.ones([len(features), len(features)])\r\n",
        "\r\n",
        "dim = model.embedding_dimension\r\n",
        "\r\n",
        "# Compute the norms of the blocks.\r\n",
        "for i in range(len(features)):\r\n",
        "  for j in range(len(features)):\r\n",
        "    block = mat[i * dim:(i + 1) * dim,\r\n",
        "                j * dim:(j + 1) * dim]\r\n",
        "    block_norm[i,j] = np.linalg.norm(block, ord=\"fro\")\r\n",
        "\r\n",
        "plt.figure(figsize=(9,9))\r\n",
        "im = plt.matshow(block_norm, cmap=plt.cm.Blues)\r\n",
        "ax = plt.gca()\r\n",
        "divider = make_axes_locatable(plt.gca())\r\n",
        "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\r\n",
        "plt.colorbar(im, cax=cax)\r\n",
        "cax.tick_params(labelsize=10) \r\n",
        "_ = ax.set_xticklabels([\"\"] + features, rotation=45, ha=\"left\", fontsize=10)\r\n",
        "_ = ax.set_yticklabels([\"\"] + features, fontsize=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29KPoi7FG0Qk"
      },
      "source": [
        "## 결과\r\n",
        "- input에 텐서플로우에서는 string자료와 int자료를 넣어서 했지만, 여기서 string 자료를 넣을때는 data를 tensor화 하지 못했다. <br>\r\n",
        "그리고 float형태의 rating per user를 input으로 넣으려 했으나 int값을 넣으라는 오류가 떴다. <br>\r\n",
        "돌아가기는 하지만 형태만 갖춘 허접한 결과값을 얻었다. 다양한 데이터들을 넣어보면서 실험 해봐야할 것같다. "
      ]
    }
  ]
}